{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+4uCLksU7kGqJG0r48xgP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vinal-2/Email-Spam-Detection---THM/blob/main/SPAM_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SPAM Detector**"
      ],
      "metadata": {
        "id": "sF-bqOheB8yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Importing the required libraries\n",
        "Before starting with Data collection, we will import the required libraries. Jupyter Notebook comes with all the libraries we need for Machine Learning. Here, we are importing two key libraries: Numpy and Pandas. These libraries are already explained in detail in the previous task.\n",
        "\n",
        "**Numpy:** NumPy (Numerical Python) is the fundamental package for numerical computations in Python.\n",
        "\n",
        "**pandas:**  Pandas provides high-level data structures and methods designed to make data analysis fast and easy in Python. It's built on top of NumPy."
      ],
      "metadata": {
        "id": "oyVxyAG6msCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "4r-Qczs2C8cg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Collection\n",
        "\n",
        "**Data collection** is the process of gathering raw data from various sources to be used for Machine Learning. This data can originate from numerous sources, such as databases, text files, APIs, online repositories, sensors, surveys, web scraping, and many others.\n",
        "\n",
        "Here, we are using the Pandas library to load the data collected from various sources in the csv format. The dataset contains spam and ham (non-spam) emails."
      ],
      "metadata": {
        "id": "cYI6cEeHEdfU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "aDmc1pIuCv05",
        "outputId": "2f4d92ba-a1a1-4525-a9bc-3648edff098c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3ad6f0c1-ac44-49b5-8fa3-785399b20d1e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3ad6f0c1-ac44-49b5-8fa3-785399b20d1e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving emails_dataset.csv to emails_dataset.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "data = pd.read_csv(\"emails_dataset.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test/Check Dataset ##\n",
        "\n",
        "Let's review the dataset we just imported. The category column contains the email classification, and the message column contains the email body, as shown below:"
      ],
      "metadata": {
        "id": "_p9X_JK1_t05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head(1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TChbYHpbDHAI",
        "outputId": "c084cfaa-4fb5-47fb-becc-9b5f96c6b2db"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Classification                                          Message\n",
            "0           spam  Congratulations !! You have won the Free ticket\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYZhzCnyH48E",
        "outputId": "45052954-ac31-48c3-9865-af7fa29721c0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Classification                                            Message\n",
            "0              spam    Congratulations !! You have won the Free ticket\n",
            "1              spam  Nah I don't think he goes to usf, he lives aro...\n",
            "2               ham  As per your request 'Melle Melle (Oru Minnamin...\n",
            "3              spam  WINNER!! As a valued network customer you have...\n",
            "4               ham  Had your mobile 11 months or more? U R entitle...\n",
            "...             ...                                                ...\n",
            "4446            ham                                                NaN\n",
            "4447            ham                                                NaN\n",
            "4448           spam                                                NaN\n",
            "4449            ham                                                NaN\n",
            "4450            ham                                                NaN\n",
            "\n",
            "[4451 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "\n",
        "Data preprocessing refers to the techniques used to convert raw data into a clean, organised, understandable, and structured format suitable for Machine Learning. Given that raw data is often messy, inconsistent, and incomplete, preprocessing is an essential step to ensure that the data feeding into the ML models is relevant and of high quality.\n",
        "\n",
        "There are several data pre-processing machine learning models, each has their own ways to process the data."
      ],
      "metadata": {
        "id": "oU37dL5dMXLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utilizing CountVectorizer()\n",
        "Machine Learning models understand numbers, not text. This means the text needs to be transformed into a numerical format. CountVectorizer, a class provided by the scikit-learn library in Python, achieves this by converting text into a token (word) count matrix. It is used to prepare the data for the Machine Learning models to use and predict decisions on.\n",
        "\n",
        "Here we are using CounterVectorizer which is used to extract featutres from the text"
      ],
      "metadata": {
        "id": "cVFQN2ULMoqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "FD9lP2CsMYfz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now use the CountVectorizer function to transform the Message column into numeric, as shown below:"
      ],
      "metadata": {
        "id": "xMcg3zLhNZyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.fillna(0, inplace=True)"
      ],
      "metadata": {
        "id": "U9Dnvl8tOaF1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(df['Message'])\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJJLar_GM76S",
        "outputId": "35639f85-faeb-476a-c400-52dba2e23eff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 1364)\t1\n",
            "  (0, 5440)\t1\n",
            "  (0, 2356)\t1\n",
            "  (0, 5357)\t1\n",
            "  (0, 4794)\t1\n",
            "  (0, 2109)\t1\n",
            "  (0, 4850)\t1\n",
            "  (1, 3279)\t1\n",
            "  (1, 1687)\t1\n",
            "  (1, 4818)\t1\n",
            "  (1, 2363)\t2\n",
            "  (1, 2228)\t1\n",
            "  (1, 4885)\t1\n",
            "  (1, 5104)\t1\n",
            "  (1, 2920)\t1\n",
            "  (1, 719)\t1\n",
            "  (1, 2391)\t1\n",
            "  (1, 4831)\t1\n",
            "  (2, 4885)\t1\n",
            "  (2, 733)\t2\n",
            "  (2, 3627)\t1\n",
            "  (2, 5441)\t3\n",
            "  (2, 4024)\t1\n",
            "  (2, 3111)\t2\n",
            "  (2, 3512)\t1\n",
            "  :\t:\n",
            "  (2672, 2980)\t1\n",
            "  (2672, 2190)\t1\n",
            "  (2672, 3937)\t1\n",
            "  (2673, 4885)\t1\n",
            "  (2673, 4888)\t1\n",
            "  (2673, 3268)\t1\n",
            "  (2673, 2627)\t1\n",
            "  (2673, 3092)\t1\n",
            "  (2673, 2390)\t1\n",
            "  (2673, 1662)\t1\n",
            "  (2673, 3701)\t3\n",
            "  (2673, 5331)\t1\n",
            "  (2673, 923)\t2\n",
            "  (2673, 1385)\t1\n",
            "  (2673, 1973)\t1\n",
            "  (2673, 3348)\t1\n",
            "  (2673, 3905)\t1\n",
            "  (2674, 4419)\t1\n",
            "  (2674, 5465)\t2\n",
            "  (2674, 737)\t1\n",
            "  (2674, 4170)\t1\n",
            "  (2674, 1372)\t1\n",
            "  (2674, 1404)\t1\n",
            "  (2674, 3026)\t1\n",
            "  (2674, 1520)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Train/Test split dataset\n",
        "It's important to test the model's performance on unseen data. By splitting the data, we can train our model on one subset and test its performance on another.\n",
        "\n",
        "Here, the variable X contains the dataset. We will use the functions from sklearn library to split the dataset into training data and testing data, as shown below:"
      ],
      "metadata": {
        "id": "k40Q3nxJMawW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "mZHfhngYMbgS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['Classification']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
      ],
      "metadata": {
        "id": "iR89v1pnOtAJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **X**: The first argument to `train_test_split` is the feature matrix `X` which you obtained from the `CountVectorizer`. This matrix contains the token counts for each message in the dataset.\n",
        "    \n",
        "- **y**: The second argument is the labels for each instance in your dataset, which indicates whether a message is spam or ham.\n",
        "    \n",
        "- **test_size=0.2**: This argument specifies that 20% of the dataset should be kept as the test set and the rest (80%) should be used for training. It's a common practice to hold out a portion of the dataset for testing to evaluate the performance of the model on unseen data.\n",
        "This is where the actual splitting of data into training and test sets happens.\n",
        "\n",
        "The function then returns four values:\n",
        "\n",
        "- **X_train**: The subset of the features to be used for training.\n",
        "- **X_test**: The subset of the features to be used for testing.\n",
        "- **y_train**: The corresponding labels for the `X_train` set.\n",
        "- **y_test**: The corresponding labels for the `X_test` set."
      ],
      "metadata": {
        "id": "Ql3I1MFnOvji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Model Training using Naive Bayes\n",
        "\n",
        "Naive Bayes is a statistical method that uses the probability of certain words appearing in spam and non-spam emails to determine whether a new email is spam or not.\n",
        "How Naive Bayes Classification Works\n",
        "\n",
        "    Let's say we have a bunch of emails, some labelled as \"spam\" and others as \"ham\".\n",
        "    The Naive Bayes algorithm learns from these emails. It looks at the words in each email and calculates how frequently each word appears in spam or ham emails. For instance, words like \"free\", \"win\", \"offer\", and \"lottery\" might appear more in spam emails.\n",
        "    The Naive Bayes algorithm calculates the probability of the email being spam based on the words it contains.\n",
        "    When the model is trained with Naive Bayes and gets a new email that says (for example) \"Win a free toy now!\", then it thinks: - \"Win\" often appears in spam, so this increases the chance of the email being spam. - \"Free\" is also common in spam, further increasing the spam probability. - \"Toy\" might be neutral, often appearing in both spam and ham. - After considering all the words, it calculates the overall probability of the email being spam and ham.\n",
        "\n",
        "If the calculated probability of spam is higher than that of ham, the algorithm classifies the email as spam. Otherwise, it's classified as ham. Let's use Naive Bayes to train the model, as shown and explained below:"
      ],
      "metadata": {
        "id": "pq1c82JaMcMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "zESvQVRwMcxm",
        "outputId": "8d6dedcb-f485-41ca-e590-7197d0f9d00b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    X_train: This is the training data you want the model to learn from. It's the token counts for each message in the training dataset, obtained from the CountVectorizer.\n",
        "    y_train: These are the correct labels (either \"spam\" or \"ham\") for each message in the X_train dataset.\n",
        "\n",
        "This is where the actual training of the model happens. The fit method is used to train or \"fit\" the model on your training data.\n"
      ],
      "metadata": {
        "id": "jw_A4b69POdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Model Evaluation\n",
        "\n",
        "After training, it's essential to evaluate the model's performance on the test set to gauge its predictive power. This will give you metrics such as accuracy, precision, and recall.\n"
      ],
      "metadata": {
        "id": "jMn5w-5OMdrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIf41Md7MeG9",
        "outputId": "02ba677f-118c-4b96-9258-f6eb36410c5e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.89      0.95      0.92       474\n",
            "        spam       0.16      0.08      0.11        61\n",
            "\n",
            "    accuracy                           0.85       535\n",
            "   macro avg       0.53      0.51      0.51       535\n",
            "weighted avg       0.81      0.85      0.82       535\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification_report function takes in the true labels (y_test) and the predicted labels (y_pred) and returns a text report showing the main classification metrics.\n",
        "\n",
        "The report gives you insights into how well your model is performing for each class and overall, in terms of these metrics.\n",
        "\n",
        "    Precision: It is the ratio of correctly predicted positive observations to the total predicted positives. The question it answers is: Of all the samples predicted as positive, how many were actually positive?\n",
        "    Recall (Sensitivity): It is the ratio of correctly predicted positive observations to all the actual positives. The question it answers is: Of all the actual positive samples, how many did we predict correctly?\n",
        "    F1-Score: It's the harmonic mean of Precision and Recall and gives a better measure of the incorrectly classified cases than the accuracy metric, especially when there's an imbalance between classes.\n",
        "    Support: It is the number of actual occurrences of the class in the specified dataset.\n",
        "    Accuracy: It's the ratio of correctly predicted observations to the total observations.\n",
        "    Macro Avg: This averages the unweighted mean per label.\n",
        "    Weighted Avg: This averages the support-weighted mean per label.\n",
        "\n",
        "\n",
        "The report gives us insights into how well your model is performing for each class and overall, in terms of these metrics."
      ],
      "metadata": {
        "id": "kAdOpIGwPjA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Testing the Model\n",
        "\n",
        "Once satisfied with the model's performance, we can use it to classify new messages and determine if they are spam or ham."
      ],
      "metadata": {
        "id": "8uGLi1hkMeSY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = vectorizer.transform([\"Today's Offer! Claim ur £150 worth of discount vouchers! Text YES to 85023 now! SavaMob, member offers mobile! T Cs 08717898035. £3.00 Sub. 16 . Unsub reply X \"])\n",
        "prediction = clf.predict(message)\n",
        "print(\"The email is :\", prediction[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bqmfd0-P243",
        "outputId": "e5f2da16-69fb-4c7e-d6bb-627447c297e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is : ham\n"
          ]
        }
      ]
    }
  ]
}